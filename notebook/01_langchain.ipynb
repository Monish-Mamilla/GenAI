{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a694652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='An attention mechanism is a crucial component in various deep learning models, particularly those using transformers like BERT, RoBERTa, and XLNet. It\\'s designed to focus on the most relevant information from the input sequence when making predictions.\\n\\nThe basic idea behind an attention mechanism is to weigh the importance of different elements within the input sequence based on their similarity to the target output (e.g., the output of a neural network). This allows the model to selectively attend to the relevant information and generate responses that are most relevant for the task at hand.\\n\\nHere\\'s a simplified overview of how an attention mechanism works:\\n\\n1. **Contextualization**: The input sequence is represented as a set of vectors, where each vector represents a particular element or \"token\" in the sequence.\\n2. **Query Representation**: A query vector is generated to represent the target output (e.g., the last word in the text). This query vector encapsulates information about what the model wants to focus on.\\n3. **Key-Value Vector Representation**: For each element in the input sequence, a key-value pair is created by concatenating the query vector with a value vector that represents the context of the element (e.g., its position and previous elements).\\n4. **Attention Weights**: A set of attention weights is computed for each element based on the dot product of the query vector with the key-value pair vectors. These weights represent how much each element contributes to the overall output.\\n5. **Normalized Attention**: The attention weights are normalized by dividing them by their sum, which helps prevent the model from paying excessive attention to certain elements that don\\'t contribute significantly to the output.\\n\\nMathematically, this can be represented as:\\n\\nAttention(p) = softmax(query^T * key / (L^2), Q)\\n\\nwhere p is the output vector of the model, q is the query vector, k is the value vector for each element, and L is the length of the input sequence.\\n\\nThe attention mechanism has several benefits:\\n\\n* **Improved performance**: By focusing on relevant information, the model can generate more accurate outputs.\\n* **Reduced overfitting**: The model is less likely to focus too much on irrelevant information, which helps prevent overfitting.\\n* **Increased flexibility**: The attention mechanism allows the model to adapt to different contexts and modify its behavior accordingly.\\n\\nHowever, the attention mechanism also introduces some challenges, such as:\\n\\n* **Computational cost**: Computing attention weights can be computationally expensive, especially for large input sequences.\\n* **Dimensionality**: The number of dimensions in the attention matrix can be high, which can lead to memory issues and performance problems if not properly optimized.\\n\\nOverall, the attention mechanism is a powerful tool for handling complex relationships between input elements and generating accurate outputs. Its versatility and flexibility make it an essential component in various deep learning applications.' response_metadata={'model': 'llama3.2:1b', 'created_at': '2025-08-16T15:41:09.573394Z', 'done': True, 'done_reason': 'stop', 'total_duration': 5405832250, 'load_duration': 944041500, 'prompt_eval_count': 32, 'prompt_eval_duration': 58672833, 'eval_count': 572, 'eval_duration': 4401964417, 'message': Message(role='assistant', content='', thinking=None, images=None, tool_name=None, tool_calls=None)} id='run-78dd0dbd-3600-4999-a6c1-261cec8d4a6b-0' usage_metadata={'input_tokens': 32, 'output_tokens': 572, 'total_tokens': 604}\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.2:1b\")\n",
    "response = llm.invoke(\"Explain how attention mechanism works.\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b32d9e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To be eligible for the Top 20 CS colleges in the USA, you typically need to meet certain prerequisites and requirements. Here are some of the common ones:\n",
      "\n",
      "1. **Computer Science or related degree**: A bachelor's or master's degree in Computer Science (CS), Information Technology (IT), Software Engineering, or a related field is usually required.\n",
      "2. **GPA**: A strong academic record with high GPAs, typically above 3.5 or 4.0, is expected. Some colleges may also consider other factors like extracurricular activities, research experience, and leadership roles.\n",
      "3. **Standardized test scores**: Many Top 20 CS colleges require or prefer applicants to submit standardized test scores, such as:\n",
      "\t* SAT (verbal reasoning, math, reading, and writing) with a minimum score of 1500 (out of 1600)\n",
      "\t* ACT (composite score of 26 or higher)\n",
      "4. **GRE (if required)**: Some colleges may require or prefer GRE scores for certain programs.\n",
      "5. **Letters of recommendation**: Two to three letters of recommendation from academic or professional references can help showcase your skills, achievements, and potential as a CS student.\n",
      "6. **Personal statement or essay**: A personal statement or essay that highlights your passion for computer science, goals, and experiences can also be required.\n",
      "7. **Portfolio or project**: Some colleges may require an official portfolio or a specific type of project (e.g., coding challenge) to demonstrate your skills and problem-solving abilities.\n",
      "8. **Interviews**: Some Top 20 CS colleges may conduct interviews with applicants as part of the admission process.\n",
      "\n",
      "Here are some additional requirements for specific schools:\n",
      "\n",
      "* University of Pennsylvania: Bachelor's degree in Computer Science, a strong GPA, and 2-3 years of work experience or internships\n",
      "* Massachusetts Institute of Technology (MIT): Bachelor's degree in Computer Science, a strong GPA, and 1-2 years of work experience or internships\n",
      "* Stanford University: Bachelor's degree in Computer Science, a strong GPA, and 1-2 years of work experience or internships\n",
      "* Carnegie Mellon University: Bachelor's degree in Computer Science, a strong GPA, and 1-2 years of work experience or internships\n",
      "\n",
      "Keep in mind that these are general guidelines, and specific requirements may vary from year to year. It's essential to check the official websites of the Top 20 CS colleges you're interested in attending for the most up-to-date information on their admission requirements.\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(\"What is prequisite to apply top 20 cs colleges in USA \")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "512a8085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain is an open-source, multi-agent programming language that enables the creation of complex and dynamic systems.\n",
      "\n",
      "It allows developers to define agents that interact with their environment in a decentralized manner.\n",
      "\n",
      "LangChain's architecture consists of several components, including the agent runtime and the knowledge graph.\n",
      "\n",
      "The knowledge graph stores and manages the relationships between different entities and concepts within the system.\n",
      "\n",
      "LangChain's programming model is designed to be flexible and extensible, enabling developers to create custom agents and systems tailored to their specific needs.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "pt = PromptTemplate.from_template(\"Write five line on {topic}, each on a new line \")\n",
    "\n",
    "chain = pt | llm \n",
    "response = chain.invoke({'topic':'langchain'})\n",
    "print(response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
